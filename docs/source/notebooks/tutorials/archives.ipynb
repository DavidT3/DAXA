{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2fc5e39",
   "metadata": {},
   "source": [
    "# Creating and interacting with a DAXA Archive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ec471d",
   "metadata": {},
   "source": [
    "This tutorial will explain the basic concepts behind the second type of important class in DAXA, the Archive class (with mission classes being the first type, see [the missions tutorial](missions.html)). DAXA Archives are what manage the datasets that we download from various missions, enabling easy access and greatly simplifying processing/reduction - they allow you to stop thinking about all the files and settings that any large dataset entails.\n",
    "\n",
    "We will cover the following:\n",
    "\n",
    "* Setting up an Archive from scratch, using filtered DAXA missions.\n",
    "* Loading an existing Archive from disk.\n",
    "* The properties of an Archive.\n",
    "* Accessing processing logs and success information (though we do not cover processing in this part of the documentation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cadd97e",
   "metadata": {},
   "source": [
    "## Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3742498",
   "metadata": {},
   "outputs": [],
   "source": [
    "from daxa.mission import XMMPointed, Chandra, eRASS1DE, ROSATPointed\n",
    "from daxa.archive import Archive\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13eeee76",
   "metadata": {},
   "source": [
    "## What is a DAXA archive?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cc2c89",
   "metadata": {},
   "source": [
    "DAXA Archives take a set of filtered missions, make sure that their data are downloaded, and enable easy access and organisation of all data files and processing functions. Key functionality includes:\n",
    "\n",
    "* Storing the logs and errors of all processing steps (if run).\n",
    "* Allowing for their easy retrieval. \n",
    "* Managing the myriad files produced during the processing.\n",
    "* Keeping track of which processes failed for which data, ensuring that any further processing only runs on data that have successfully passed through the earlier processes.\n",
    "\n",
    "Archives can also be loaded back into DAXA at a later date, so that the processing logs of data that has since been found to be problematic can be easily inspected, or indeed so that processing steps can be re-run with different settings; this also allows for archives to be updated, if more data become available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03822d2",
   "metadata": {},
   "source": [
    "## Creating a new archive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc71354",
   "metadata": {},
   "source": [
    "Here we will demonstrate how to set up a new DAXA Archive from scratch - this information can be combined with the [the missions tutorial](missions.html) and the [case studies](../../tutorials.casestudies.html) to create an archive from any dataset you might be using."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2587cb4",
   "metadata": {},
   "source": [
    "### Step 1 - Set up and filter missions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade371be",
   "metadata": {},
   "source": [
    "The first thing we have to do is to select the observations that we wish to include in the archive (and indeed the missions that we wish to include). The missions all have different characteristics, so your choice of which to include will be heavily dependent on your science case.\n",
    "\n",
    "Here we will create an archive of XMM, Chandra, eROSITA All-Sky DR1, and ROSAT pointed observations of a famous galaxy cluster (though the archive would behave the same if it held data for a large sample of objects).\n",
    "\n",
    "First of all, we define instances of the mission classes that we wish to include:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c420fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dt237/code/DAXA/daxa/mission/xmm.py:83: UserWarning: 140 of the 17701 observations located for this mission have been removed due to NaN RA or Dec values\n",
      "  self._fetch_obs_info()\n"
     ]
    }
   ],
   "source": [
    "xm = XMMPointed()\n",
    "ch = Chandra()\n",
    "er = eRASS1DE()\n",
    "rp = ROSATPointed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60065cc6",
   "metadata": {},
   "source": [
    "Then we filter them to only include observations of our cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04ddc813",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dt237/code/DAXA/daxa/mission/base.py:1335: UserWarning: Chandra FoV are difficult to define, as they can be strongly dependant on observation mode; as such take these as very approximate.\n",
      "  fov = self.fov\n"
     ]
    }
   ],
   "source": [
    "xm.filter_on_name(\"A3667\")\n",
    "ch.filter_on_name(\"A3667\")\n",
    "er.filter_on_name(\"A3667\")\n",
    "rp.filter_on_name(\"A3667\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf1dc67",
   "metadata": {},
   "source": [
    "We then download the available data (though the declaration of an Archive would also trigger this, we do it this way because we wish to download pre-generated products for Chandra and ROSAT pointed observations):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31d6ba22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading XMM-Newton Pointed data: 100%|██████████████████████████████████████| 8/8 [01:24<00:00, 10.52s/it]\n",
      "Downloading Chandra data: 100%|███████████████████████████████████████████████| 12/12 [02:37<00:00, 13.14s/it]\n",
      "Downloading ROSAT Pointed data: 100%|███████████████████████████████████████████| 3/3 [00:16<00:00,  5.62s/it]\n"
     ]
    }
   ],
   "source": [
    "xm.download()\n",
    "ch.download(download_products=True)\n",
    "er.download()\n",
    "rp.download(download_products=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c8d61c",
   "metadata": {},
   "source": [
    "### Step 2 - Setting up an Archive object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbeb6b0",
   "metadata": {},
   "source": [
    "Now we create the actual DAXA Archive instance - we can pass the following arguments:\n",
    "\n",
    "* `archive_name` - The name to be given to the archive (used to load it back in at a later date, if necessary). The only input required\n",
    "* `missions` - The filtered missions that we have already created (can be left as None if loading in an existing archive).\n",
    "* `clobber` - Will overwrite an existing archive if the passed `archive_name` has already been used. Default is False.\n",
    "* `download_products` - If the missions have not already had downloads triggered, this controls whether pre-processed products should be downloaded or not. Default is True, a dictionary with mission names as keys and True/False as values can be passed to provide more nuanced control.\n",
    "* `use_preprocessed` - Whether pre-processed data (for those missions that provide it) should be automatically imported into the archive processed data structure. Default is False, a dictionary with mission names as keys and True/False as values can be passed to provide more nuanced control.\n",
    "\n",
    "We demonstrate how to set up an archive using the pre-processed data that is downloadable from the Chandra, eRASS1DE, and ROSAT-Pointed online datasets, as well as the raw data available from the XMM online dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85477362",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Including pre-processed Chandra data in the archive: 100%|████████████████████| 12/12 [00:00<00:00, 26.06it/s]\n",
      "Including pre-processed ROSAT Pointed data in the archive: 100%|████████████████| 3/3 [00:00<00:00, 18.00it/s]\n"
     ]
    }
   ],
   "source": [
    "arch = Archive(\"A3667\", [xm, ch, er, rp], clobber=True, \n",
    "               use_preprocessed={'xmm_pointed': False, 'chandra': True, \n",
    "                                 'erosita_all_sky_de_dr1': True, \n",
    "                                 'rosat_pointed': True})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c9ea52",
   "metadata": {},
   "source": [
    "Now we've declared it, we can use the `info()` method to get a summary of its current status, including the amount of data available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8be6d301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------------------------------------------------\n",
      "Number of missions - 3\n",
      "Total number of observations - 23\n",
      "Beginning of earliest observation - 1992-04-14 18:55:38.000003\n",
      "End of latest observation - 2015-07-19 23:37:58.999997\n",
      "\n",
      "-- XMM-Newton Pointed --\n",
      "   Internal DAXA name - xmm_pointed\n",
      "   Chosen instruments - M1, M2, PN\n",
      "   Number of observations - 8\n",
      "   Fully Processed - False\n",
      "\n",
      "-- Chandra --\n",
      "   Internal DAXA name - chandra\n",
      "   Chosen instruments - ACIS-I, ACIS-S, HRC-I, HRC-S\n",
      "   Number of observations - 12\n",
      "   Fully Processed - True\n",
      "\n",
      "-- ROSAT Pointed --\n",
      "   Internal DAXA name - rosat_pointed\n",
      "   Chosen instruments - PSPCB, PSPCC, HRI\n",
      "   Number of observations - 3\n",
      "   Fully Processed - True\n",
      "-----------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "arch.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1703f4d",
   "metadata": {},
   "source": [
    "### Step 3 - Processing the Archive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7666408f",
   "metadata": {},
   "source": [
    "We're not actually going to cover _how_ to process things here, as each telescope tends to have its own backend software with a unique way of doing things; they each have their own processing tutorials, which will demonstrate both a one-line processing method, and how to control the reduction in more detail. Any processing method will take the archive object as an argument, and act on the data stored within it.\n",
    "\n",
    "So instead we include this step here to highlight that the next logical step after the creation of a new archive is to run processing and reduction routines, if raw data have been downloaded. The successful completion of this step will leave you with an archive of data that you can easily manage, access, and use for your scientific analyses.\n",
    "\n",
    "If you elected to download existing products (most missions support this), then only one processing step is necessary - this reorganises the downloaded data so that it is compatible with DAXA storage and file naming conventions. **It will have run automatically on declaration**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755de07e",
   "metadata": {},
   "source": [
    "### Step 4 - Using the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1252eb87",
   "metadata": {},
   "source": [
    "Once an archive has been constructed, whether of raw data to be processed locally, pre-processed data products, or a combination of both, the end result will be a set of X-ray data that can be used for scientific analysis. \n",
    "\n",
    "The storage and organisation of the archive's data is entirely consistent between different telescopes; the archive's processed data (the 'processed_data' directory within the overall archive path, which is identified using the `archive_path` property) is organised like this:\n",
    "\n",
    "* **Mission Name** - For instance, 'xmm_pointed' or 'chandra'\n",
    "    * _ObsID_ - Each ObsID of a mission gets a sub-directory.\n",
    "        * events - Where event lists (uncleaned, cleaned, and final) are stored.\n",
    "        * images - Where all images and exposure maps are stored.\n",
    "        * background - Where any background maps or models are stored.\n",
    "        * cleaning - Where any by-products of the cleaning processes are stored.\n",
    "        * logs - Where logs from all processing steps are stored.\n",
    "\n",
    "To provide an example, we show you the contents of an eRASS1DE directory that contains pre-processed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bdb6e0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['background', 'images', 'logs', 'events', 'cleaning']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_pth = arch.get_current_data_path('erosita_all_sky_de_dr1', '304147')\n",
    "\n",
    "os.listdir(demo_pth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfea6adf",
   "metadata": {},
   "source": [
    "Now the contents of the images sub-directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1494a78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['obsid6292-instACIS-I-subexpNone-en0.5_7.0keV-image.fits']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(os.path.join(demo_pth, 'images'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b3f951",
   "metadata": {},
   "source": [
    "As well as the events sub-directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "274c0fc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['obsid6292-instACIS-I-subexpNone-finalevents.fits']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(os.path.join(demo_pth, 'events'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ded0b1",
   "metadata": {},
   "source": [
    "The common format of the filenames used by DAXA should be fairly evident by this point; it extends across all data products produced by DAXA and is shared across all the different supported missions. Some elements may differ between data products, but the overall structure is the same.\n",
    "\n",
    "Each name contains summary information that allow it to be uniquely identified, both programatically and by a user; different pieces of information start with an identifier (for instance obsid to denote where the relevant ObsID begins), and different pieces are separated by a '-'. If there are multiple bits of information connected to a particular category then they are separated by '\\_' (see 'inst' part of eROSITA file names above).\n",
    "\n",
    "The following can be contained in a filename:\n",
    "\n",
    "* **ObsID** - Denoted by 'obsid', this will be present in the name of every data file.\n",
    "* **Instrument** - Denoted by 'inst', this should be present in the name of every data file.\n",
    "* **Sub-exposure ID** - Denoted by 'subexp'; some missions support multiple sub-exposures per ObsID instrument, in which case the sub-exposure ID will be included here. It is also possible for 'None' to be the value if there are no sub-exposures, or 'ALL' if the file is a combination of sub-exposures.\n",
    "* **Lower and Upper Energy** - Denoted by 'en', this will be of the form en{lower}\\_{upper}{unit} (en0.2\\_10.0keV for instance). This is for cases where the product is energy bound. It is also possible for the value to be 'None' if the 'en' identifier is included in the filename but no energy limits were applied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28746332",
   "metadata": {},
   "source": [
    "### Note on saving Archives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a48c86",
   "metadata": {},
   "source": [
    "Archive instances can be saved and loaded back in (as you'll see in the next section). This can be triggered manually by running the `save()` method, but __this shouldn't be generally necessary__ - this is because the archive is automatically saved upon first setup, and after every processing step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b1269a",
   "metadata": {},
   "source": [
    "## Loading an existing archive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44310223",
   "metadata": {},
   "source": [
    "As we have intimated, previously created archives can be loaded back in to memory in exactly the same state as when they were saved. We will demonstrate this here with an archive we prepared earlier - it has had XMM processing applied, which will allow us to demonstrate the logging and management functionality. \n",
    "\n",
    "Reloading an archive has a number of possible applications:\n",
    "\n",
    "* Access to archive data management functions - e.g. locating specific data files, identifying what observations are available.\n",
    "* Checking processing logs - e.g. finding errors or warnings in the processing of data that has since been identified as problematic.\n",
    "* Updating the archive - either adding another mission, or using the archive to check for new data matching your original mission filtering operations (these are stored in the mission saves, so can be re-run automatically).\n",
    "\n",
    "All you need to do is set up an Archive instance and pass the name of an existing archive - this assumes your code is running in the same directory as it was originally, as Archives are stored in 'daxa_output' (if the DAXA configuration file hasn't been altered). The configuration can also be altered so that all DAXA outputs are stored in an absolute path, in which case defining an Archive object with the name of an existing dataset would work from any directory).\n",
    "\n",
    "Loading in an archive (note that you don't need to pass any missions, loading the archive back in will also reinstate the missions as they were when the Archive was last saved):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8308abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_arch = Archive(\"PHL1811_made_earlier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5a421e",
   "metadata": {},
   "source": [
    "Once again, we will run the `info()` method, but note that for this archive the XMM-Newton Pointed mission is marked as 'fully processed':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4645c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_arch.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21da27ce",
   "metadata": {},
   "source": [
    "We note that it _is_ possible to declare an Archive with a previously used name and overwrite it - you just have to pass `clobber=True` when you declare the Archive instance. We print the docstring of the Archive class here for reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d65a63",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(Archive.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69f2746",
   "metadata": {},
   "source": [
    "## Accessing component missions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274ef587",
   "metadata": {},
   "source": [
    "The missions that were used to create an archive can be retrieved, giving access to their information tables - note that you cannot just use the filtering methods of a mission to change the data in the archive; altering the observations in an archive requires using <font color='red'>the archive `update()` method.</font>\n",
    "\n",
    "To retrieve a mission you can either address the archive with the DAXA internal name of the mission, or get the whole list using the `missions` property:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572e9230",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_arch['xmm_pointed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9670a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_arch.missions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f08d7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_arch['xmm_pointed'].filtered_obs_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c60fde",
   "metadata": {},
   "source": [
    "## Archive general properties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6101cd2e",
   "metadata": {},
   "source": [
    "Here we run through the general properties of the archive class, summarising their meaning and content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b605eaf1",
   "metadata": {},
   "source": [
    "### Name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8334d258",
   "metadata": {},
   "source": [
    "The `archive_name` class returns the name that was given to the archive on creation - this cannot be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a03264",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_arch.archive_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dddccd",
   "metadata": {},
   "source": [
    "### Archive Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f738f30d",
   "metadata": {},
   "source": [
    "This property (`archive_path`) returns the absolute path to the top level of this archive's storage directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4902e739",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_arch.archive_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbcbe0e",
   "metadata": {},
   "source": [
    "### Mission Names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a126d0",
   "metadata": {},
   "source": [
    "In addition to the `missions` property discussed earlier, we include a `mission_names` property which lists the internal names of the mission classes associated with the archive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf14f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_arch.mission_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a1019d",
   "metadata": {},
   "source": [
    "## Processing-related Archive properties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51f86da",
   "metadata": {},
   "source": [
    "This section deals with Archive properties that are related to processing of the available data into something scientifically useful - this is why we loaded an existing archive with processing applied, to demonstrate the contents of these properties:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a98a1cd",
   "metadata": {},
   "source": [
    "### Process Success"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598f2fc6",
   "metadata": {},
   "source": [
    "The `process_success` property is very important - it is a nested dictionary which records which processing steps were 'successful' (usually defined as no errors being detected, and expected files being found) for which data. Those that were successful have a boolean value of True, those that weren't have a boolean value of False.\n",
    "\n",
    "Top level keys will always be mission name, the next level down will be the process name, and the layer below that will be the unique IDs of the data the process acted on. This is often an ObsID, but can also be ObsID + instrument name, or ObsID + instrument name + sub-exposure ID.\n",
    "\n",
    "This allows you (but more importantly the archive itself) to know which stages have failed for which data - that in turn means any processing that is dependent on a previous stage can know which data to skip. All this ensures no interruptions when reducing large sets of data.\n",
    "\n",
    "In this case we've run all processing steps on the XMM data in the archive, note the following entry:\n",
    "\n",
    "* `espfilt` - 0502671101PNS003 \n",
    "\n",
    "It failed safely, and was not considered for the next processing stages. **Also note that the ObsID 0102041001 does not appear** after the `odf_ingest` step, as all of its data was taken in CalClosed mode and can't be used for the study of the target objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5b9554",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_arch.process_success"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1703b61",
   "metadata": {},
   "source": [
    "### Process Names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a8621d",
   "metadata": {},
   "source": [
    "This property (`process_names`) stores a list of the processes that have been run on each mission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573dce86",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_arch.process_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cbd34e",
   "metadata": {},
   "source": [
    "### Process Logs (stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e9a586",
   "metadata": {},
   "source": [
    "All of the logs for all processes run by DAXA are stored, and can be accessed through the `process_logs` property - this is structured in the exact same way as `process_success`, as a nested dictionary. The only difference here is that the final values are strings rather than booleans.\n",
    "\n",
    "We show the log for a single process applied to a single piece of data, otherwise this tutorial document would be very long indeed - this particular process worked perfectly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c07af8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(prev_arch.process_logs['xmm_pointed']['espfilt']['0204310101M1S001'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5c59d9",
   "metadata": {},
   "source": [
    "### Process Raw Errors (stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3003fb8e",
   "metadata": {},
   "source": [
    "Related to the previous section is the `raw_process_errors` property, which stores the stderr output of any process that generated one - note that if no stderr was produced, then we do not create an entry. DAXA makes a distinction between 'raw process errors' and the 'process errors' you'll see in the next section - this is because we attempt to parse any stderr and extract particular errors, versus this property which is just the raw text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cd0978",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_arch.raw_process_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55db330",
   "metadata": {},
   "source": [
    "### Process Parsed Errors (parsed from stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c0e8cf",
   "metadata": {},
   "source": [
    "We attempt to extract the pertinent information from the raw error outputs, and this is what gets stored in `process_errors` - again with the same storage structure. Here we just show a single entry, for one of the pieces of data that we noted had failed a processing step in the `process_success` section of this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e001af",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_arch.process_errors['xmm_pointed']['espfilt']['0502671101PNS003']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8216a10",
   "metadata": {},
   "source": [
    "### Process Parsed Warnings (parsed from stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72337d2",
   "metadata": {},
   "source": [
    "We make a distinction between errors and warnings, as do many pieces of backend software. The `process_warnings` property acts exactly as the `process_errors` property, but it is warnings that have been extracted rather than errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a002966",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "prev_arch.process_warnings['xmm_pointed']['epchain'][\"0502671101PNS003\"][109]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb21cec2",
   "metadata": {},
   "source": [
    "### Process Extra Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f87cdb",
   "metadata": {},
   "source": [
    "The `process_extra_info` property is something of a catch-all for any information passed into, or produced by, a processing step that the archive might need access to later on. This can include things like the paths to the key output files of a process. This isn't really meant to be useful to the user, but can still be accessed.\n",
    "\n",
    "We only show the contents for one processing step (`cleaned_evt_lists`) so as not to make the tutorial too long:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee54699",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_arch.process_extra_info['xmm_pointed']['cleaned_evt_lists']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6500150",
   "metadata": {},
   "source": [
    "### Final process success"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5694931",
   "metadata": {},
   "source": [
    "This property (`final_process_success`) is the final arbiter of whether a particular ObsID of a mission can be used by the end user - this is only decided when whatever DAXA defines as the 'final processing step' for a particular mission is run (for XMM this is the assembly of cleaned event lists). \n",
    "\n",
    "If it is False, then none of the data for that ObsID reached the final processing step - note here that 0102041001 is False, this is because all the instruments were in calibration mode with the filter closed. As such, this ObsID is moved from the 'processed_data' directory in the archive storage structure, to the 'failed_data' directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b67d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_arch.final_process_success"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7718d0bb",
   "metadata": {},
   "source": [
    "### Observation Summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2bb513",
   "metadata": {},
   "source": [
    "This property (`observation_summaries`) contains a summary of the exact data available for particular observations of a mission, and is meant to allow DAXA processes to access whether they can run on a particular ObsID (not really relevant to end users). The property is populated in different ways for each mission - the XMMPointed mission, for instance, parses the output summary file from `odf_ingest` and turns it into an extremely detailed summary of the state of each instrument on XMM for an observation. \n",
    "\n",
    "For clarity we only show the output for one ObsID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fb0787",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "prev_arch.observation_summaries['xmm_pointed']['0204310101']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cb12b6",
   "metadata": {},
   "source": [
    "## Data management Archive functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ebca8f",
   "metadata": {},
   "source": [
    "This section introduces the built-in methods that allow you to manage the archive and its data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dad937",
   "metadata": {},
   "source": [
    "### Get path to data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40397b87",
   "metadata": {},
   "source": [
    "One of the most useful methods of the archive class is `get_current_data_path()`, which allows you to programmatically retrieve the current path to a particular ObsID's data in the archive storage structure (this takes into account the `final_process_success` flag - remember that entirely failed data are moved to a 'failed_data' directory.\n",
    "\n",
    "All we need to do is pass the mission name and the ObsID, and the top-level data path will be returned. Here we show two examples, one for an ObsID with a True final success flag, and one with a False final success flag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc83029",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_arch.get_current_data_path('xmm_pointed', '0204310101')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd206c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_arch.get_current_data_path('xmm_pointed', '0102041001')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac85761b",
   "metadata": {},
   "source": [
    "### Get failed processes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd63eaf",
   "metadata": {},
   "source": [
    "Often we will wish to know exactly which data failed a particular processing step - this could be inferred from the `process_success` property, but we also provide a convenient get method (`get_failed_processes()`) that will retrieve the unique identifer of each piece of data that failed a specified processing step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430f828b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_arch.get_failed_processes('espfilt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb8fa8f",
   "metadata": {},
   "source": [
    "### Get logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f325302d",
   "metadata": {},
   "source": [
    "The `get_process_logs()` method provides a more convenient way of accessing specific logs stored in the `process_logs` property. It allows for the retrieval of logs based on several criteria, with the only requirement being the passing of a process name. \n",
    "\n",
    "Beyond that you can specify the mission name, ObsID, and instrument to retrieve logs for - as for some processes there are sub-exposures of a given instrument, giving this information can still result in multiple logs being returned. You can also pass __lists__ of mission name, ObsID, and instrument and retrieve sets of logs that way.\n",
    "\n",
    "It is also possible to specify an exact unique identifier (0502671101M2S002 for instance).\n",
    "\n",
    "We demonstrate fetching the `espfilt` logs for a single PN instrument of 0502671101:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5195e7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "prev_arch.get_process_logs('espfilt', obs_id='0502671101', inst='PN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e1e4c6",
   "metadata": {},
   "source": [
    "### Get raw errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cda0496",
   "metadata": {},
   "source": [
    "There is also an equivalent method, `get_process_raw_error_logs()` that can fetch raw error logs. It behaves exactly the same as `get_process_logs()`, see the last section for information on the possible arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0434e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_arch.get_process_raw_error_logs('espfilt', obs_id='0502671101', inst='PN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77530ba3",
   "metadata": {},
   "source": [
    "### Get failed logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8750ea0",
   "metadata": {},
   "source": [
    "There is one final method that allows for log retrieval - `get_failed_logs()`. This only takes a process name as an argument, and will retrieve the logs and raw error logs for every piece of data that failed the specified processing step. It returns them as a tuple, with the first entry being the dictionary of logs and the second being the dictionary of raw errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76d5d2b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "logs, errors = prev_arch.get_failed_logs('espfilt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bda78b",
   "metadata": {},
   "source": [
    "We only show 0502671101M2S002:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5fb4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs['xmm_pointed']['0502671101PNS003']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828b4b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02eee7f2",
   "metadata": {},
   "source": [
    "## Adding region files to the Archive (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc74ab12",
   "metadata": {},
   "source": [
    "Region files are created by running source detection algorithms on images generated from X-ray observations, and DAXA **does not yet have the ability to generate them itself**. However, they are a crucial part of many analyses, and as such a crucial part of data archives.\n",
    "\n",
    "If you have created region files (**in the DS9 format**), you can add them to the archive so that they are included in the storage structure. This process verifies that the passed region files are in RA-Dec coordinates (rather than defined in the pixel coordinates of the image they were detected in), by requiring that images or WCS information be passed in those circumstances.\n",
    "\n",
    "* {'mission_name': {'ObsID': 'path to regions'}}\n",
    "* {'mission_name': {'ObsID': [list of region objects]}}\n",
    "* {'mission_name': {'ObsID': {'region': 'path to regions'}}}\n",
    "* {'mission_name': {'ObsID': {'region': [list of region objects]}}}\n",
    "* {'mission_name': {'ObsID': {'region': ..., 'wcs_src': 'path to image'}}}\n",
    "* {'mission_name': {'ObsID': {'region': ..., 'wcs_src': XGA Image}}}\n",
    "* {'mission_name': {'ObsID': {'region': ..., 'wcs_src': Astropy WCS object}}}\n",
    "\n",
    "For instance, we have used a tool to generate source regions for the observations we have processed, and want to add those regions to the archive. As the region files are in pixel coordinates we pass the paths to the images we used (unnecessary if regions are already in RA-Dec coordinates):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77621ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing the region files in the test directory\n",
    "reg_files = os.listdir('region_files')\n",
    "\n",
    "# Setting up the structure of the dictionary we will pass to the archive at the end of this\n",
    "reg_paths = {'xmm_pointed': {}}\n",
    "# Iterating through the ObsIDs in the XMMPointed mission\n",
    "for oi in prev_arch['xmm_pointed'].filtered_obs_ids:\n",
    "    # Checking to see which have a corresponding region file\n",
    "    if any([oi in rf for rf in reg_files]):\n",
    "        # Generating the path to the image we need for pixel to RA-Dec conversion\n",
    "        im_pth = prev_arch.get_current_data_path('xmm_pointed', oi) + \\\n",
    "        'images/{}_mos1_0.5-2.0keVimg.fits'.format(oi)\n",
    "        # Setting up the entry in the final dictionary, with the path to the regions and the image\n",
    "        reg_paths['xmm_pointed'][oi] = {'regions': 'region_files/{}.reg'.format(oi), 'wcs_src': im_pth}\n",
    "\n",
    "# Adding the regions to the archive\n",
    "prev_arch.source_regions = reg_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5432baef",
   "metadata": {},
   "source": [
    "Once added to the archive, you can also retrieve the regions through a property (we did not discuss it earlier in this tutorial) - `source_regions`. It returns them as Python 'regions' module objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea747d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_arch.source_regions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
