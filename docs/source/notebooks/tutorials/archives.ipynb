{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2fc5e39",
   "metadata": {},
   "source": [
    "# Creating and interacting with a DAXA Archive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ec471d",
   "metadata": {},
   "source": [
    "This tutorial will explain the basic concepts behind the second type of important class in DAXA, the Archive class (with mission classes being the first type, see [the missions tutorial](missions.html)). DAXA Archives are what manage the datasets that we download from various missions, enabling easy access and greatly simplifying processing/reduction - they allow you to stop thinking about all the files and settings that any large dataset entails.\n",
    "\n",
    "We will cover the following:\n",
    "\n",
    "* Setting up an Archive from scratch, using filtered DAXA missions.\n",
    "* Loading an existing Archive from disk.\n",
    "* The properties of an Archive.\n",
    "* Accessing processing logs and success information (though we do not cover processing in this part of the documentation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cadd97e",
   "metadata": {},
   "source": [
    "## Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "612438e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from daxa.mission import XMMPointed, Chandra, eRASS1DE, ROSATPointed\n",
    "from daxa.archive import Archive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13eeee76",
   "metadata": {},
   "source": [
    "## What is a DAXA archive?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cc2c89",
   "metadata": {},
   "source": [
    "DAXA Archives take a set of filtered missions, make sure that their data are downloaded, and enable easy access and organisation of all data files and processing functions. Key functionality includes:\n",
    "\n",
    "* Storing the logs and errors of all processing steps (if run).\n",
    "* Allowing for their easy retrieval. \n",
    "* Managing the myriad files produced during the processing.\n",
    "* Keeping track of which processes failed for which data, ensuring that any further processing only runs on data that have successfully passed through the earlier processes.\n",
    "\n",
    "Archives can also be loaded back into DAXA at a later date, so that the processing logs of data that has since been found to be problematic can be easily inspected, or indeed so that processing steps can be re-run with different settings; this also allows for archives to be updated, if more data become available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03822d2",
   "metadata": {},
   "source": [
    "## Creating a new archive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc71354",
   "metadata": {},
   "source": [
    "Here we will demonstrate how to set up a new DAXA Archive from scratch - this information can be combined with the [the missions tutorial](missions.html) and the <font color='red'>case studies</font> to create an archive from any dataset you might be using."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10f79f4",
   "metadata": {},
   "source": [
    "### Step 1 - Set up and filter missions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6c63e3",
   "metadata": {},
   "source": [
    "The first thing we have to do is to select the observations that we wish to include in the archive (and indeed the missions that we wish to include). The missions all have different characteristics, so your choice of which to include will be heavily dependent on your science case.\n",
    "\n",
    "Here we will create an archive of XMM, Chandra, eROSITA All-Sky DR1, and ROSAT pointed observations of a famous galaxy cluster (though the archive would behave the same if it held data for a large sample of objects).\n",
    "\n",
    "First of all, we define instances of the mission classes that we wish to include:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c420fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dt237/code/DAXA/daxa/mission/xmm.py:83: UserWarning: 140 of the 17678 observations located for this mission have been removed due to NaN RA or Dec values\n",
      "  self._fetch_obs_info()\n"
     ]
    }
   ],
   "source": [
    "xm = XMMPointed()\n",
    "ch = Chandra()\n",
    "er = eRASS1DE()\n",
    "rp = ROSATPointed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13c448e",
   "metadata": {},
   "source": [
    "Then we filter them to only include observations of our cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "700b1011",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dt237/code/DAXA/daxa/mission/base.py:1071: UserWarning: Chandra FoV are difficult to define, as they can be strongly dependant on observation mode; as such take these as very approximate.\n",
      "  fov = self.fov\n"
     ]
    }
   ],
   "source": [
    "xm.filter_on_name(\"A3667\")\n",
    "ch.filter_on_name(\"A3667\")\n",
    "er.filter_on_name(\"A3667\")\n",
    "rp.filter_on_name(\"A3667\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a932a24",
   "metadata": {},
   "source": [
    "We then download the available data (though the declaration of an Archive would also trigger this, we do it this way because we wish to download pre-generated products for Chandra and ROSAT pointed observations):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e008893c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/td/gw9qkx6d3szb1nkt_cfvcbzm000vzl/T/ipykernel_39574/3764573067.py:1: UserWarning: The raw data for this mission have already been downloaded.\n",
      "  xm.download()\n",
      "Downloading Chandra data: 100%|███████████████████████████████████████████████| 12/12 [03:28<00:00, 17.38s/it]\n",
      "Downloading eRASS DE:1 data: 100%|██████████████████████████████████████████████| 1/1 [00:05<00:00,  5.54s/it]\n",
      "Downloading ROSAT Pointed data: 100%|███████████████████████████████████████████| 3/3 [00:19<00:00,  6.36s/it]\n"
     ]
    }
   ],
   "source": [
    "xm.download()\n",
    "ch.download(download_products=True)\n",
    "er.download()\n",
    "rp.download(download_products=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc26ff4c",
   "metadata": {},
   "source": [
    "### Step 2 - Setting up an Archive object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a09765",
   "metadata": {},
   "source": [
    "Now we create the actual DAXA Archive instance - all this requires is for us to choose an archive name (which is what will be used to load it back in at a later date, if necessary) and to pass in the filtered missions that we have already created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08a25282",
   "metadata": {},
   "outputs": [],
   "source": [
    "arch = Archive(\"A3667\", [xm, ch, er, rp])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17299671",
   "metadata": {},
   "source": [
    "Now we've declared it, we can use the `info()` method to get a summary of its current status, including the amount of data available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52f73124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------------------------------------------------\n",
      "Number of missions - 4\n",
      "Total number of observations - 24\n",
      "Beginning of earliest observation - 1992-04-14 18:55:38.000003\n",
      "End of latest observation - 2020-04-20 12:23:50\n",
      "\n",
      "-- XMM-Newton Pointed --\n",
      "   Internal DAXA name - xmm_pointed\n",
      "   Chosen instruments - M1, M2, PN\n",
      "   Number of observations - 8\n",
      "   Fully Processed - False\n",
      "\n",
      "-- Chandra --\n",
      "   Internal DAXA name - chandra\n",
      "   Chosen instruments - ACIS-I, ACIS-S, HRC-I, HRC-S\n",
      "   Number of observations - 12\n",
      "   Fully Processed - False\n",
      "\n",
      "-- eRASS DE:1 --\n",
      "   Internal DAXA name - erosita_all_sky_de_dr1\n",
      "   Chosen instruments - TM1, TM2, TM3, TM4, TM5, TM6, TM7\n",
      "   Number of observations - 1\n",
      "   Fully Processed - False\n",
      "\n",
      "-- ROSAT Pointed --\n",
      "   Internal DAXA name - rosat_pointed\n",
      "   Chosen instruments - PSPCB, PSPCC, HRI\n",
      "   Number of observations - 3\n",
      "   Fully Processed - False\n",
      "-----------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "arch.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920154df",
   "metadata": {},
   "source": [
    "### Step 3 - Processing the Archive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded35b2a",
   "metadata": {},
   "source": [
    "We're not actually going to cover _how_ to process things here, as each telescope tends to have its own backend software with a unique way of doing things; they each have their own processing tutorials, which will demonstrate both a one-line processing method, and how to control the reduction in more detail. Any processing method will take the archive object as an argument, and act on the data stored within it.\n",
    "\n",
    "So instead we include this step here to highlight that the next logical step after the creation of a new archive is to run processing and reduction routines, if raw data have been downloaded. The successful completion of this step will leave you with an archive of data that you can easily manage, access, and use for your scientific analyses.\n",
    "\n",
    "If you elected to download existing products (most missions support this), then only one processing step is necessary - this reorganises the downloaded data so that it is compatible with DAXA storage and file naming conventions. **It will have run automatically on declaration**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b1269a",
   "metadata": {},
   "source": [
    "## Loading an existing archive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44310223",
   "metadata": {},
   "source": [
    "As we have intimated, previously created archives can be loaded back in to memory in exactly the same state as when they were saved. We will demonstrate this here with an archive we prepared earlier - it has had XMM processing applied, which will allow us to demonstrate the logging and management functionality. \n",
    "\n",
    "Reloading an archive has a number of possible applications:\n",
    "\n",
    "* Access to archive data management functions - e.g. locating specific data files, identifying what observations are available.\n",
    "* Checking processing logs - e.g. finding errors or warnings in the processing of data that has since been identified as problematic.\n",
    "* Updating the archive - either adding another mission, or using the archive to check for new data matching your original mission filtering operations (these are stored in the mission saves, so can be re-run automatically).\n",
    "\n",
    "All you need to do is set up an Archive instance and pass the name of an existing archive - this assumes your code is running in the same directory as it was originally, as Archives are stored in 'daxa_output' (if the DAXA configuration file hasn't been altered). The configuration can also be altered so that all DAXA outputs are stored in an absolute path, in which case defining an Archive object with the name of an existing dataset would work from any directory).\n",
    "\n",
    "Loading in an archive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b43e44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50f2ee45",
   "metadata": {},
   "source": [
    "We note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f6468cf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Archive in module daxa.archive.base:\n",
      "\n",
      "class Archive(builtins.object)\n",
      " |  Archive(archive_name: str, missions: Union[List[daxa.mission.base.BaseMission], daxa.mission.base.BaseMission] = None, clobber: bool = False)\n",
      " |  \n",
      " |  The Archive class, which is to be used to consolidate and provide some interface with a set\n",
      " |  of mission's data. Archives can be passed to processing and cleaning functions in DAXA, and also\n",
      " |  contain convenience functions for accessing summaries of the available data.\n",
      " |  \n",
      " |  :param str archive_name: The name to be given to this archive - it will be used for storage\n",
      " |      and identification. If an existing archive with this name exists it will be read in, unless clobber=True.\n",
      " |  :param List[BaseMission]/BaseMission missions: The mission, or missions, which are to be included\n",
      " |      in this archive - any setup processes (i.e. the filtering of data to be acquired) should be\n",
      " |      performed prior to creating an archive. The default value is None, but this should be set for any new\n",
      " |      archives, it can only be left as None if an existing archive is being read back in.\n",
      " |  :param bool clobber: If an archive named 'archive_name' already exists, then setting clobber to True\n",
      " |      will cause it to be deleted and overwritten.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __delitem__(self, key: Union[int, str])\n",
      " |      This deletes a mission from the archive.\n",
      " |      \n",
      " |      :param int/str key: The index or name of the mission to delete.\n",
      " |  \n",
      " |  __getitem__(self, key: Union[int, str]) -> daxa.mission.base.BaseMission\n",
      " |      This returns the relevant mission when an archive is addressed using the name of a mission as\n",
      " |      the key, or using an integer index.\n",
      " |      \n",
      " |      :param int/str key: The index or name of the mission to fetch.\n",
      " |      :return: The relevant mission object.\n",
      " |      :rtype: BaseMission\n",
      " |  \n",
      " |  __init__(self, archive_name: str, missions: Union[List[daxa.mission.base.BaseMission], daxa.mission.base.BaseMission] = None, clobber: bool = False)\n",
      " |      The init of the Archive class, which is to be used to consolidate and provide some interface with a set\n",
      " |      of mission's data. Archives can be passed to processing and cleaning functions in DAXA, and also\n",
      " |      contain convenience functions for accessing summaries of the available data.\n",
      " |      \n",
      " |      :param str archive_name: The name to be given to this archive - it will be used for storage\n",
      " |          and identification. If an existing archive with this name exists it will be read in, unless clobber=True.\n",
      " |      :param List[BaseMission]/BaseMission missions: The mission, or missions, which are to be included\n",
      " |          in this archive - any setup processes (i.e. the filtering of data to be acquired) should be\n",
      " |          performed prior to creating an archive. The default value is None, but this should be set for any new\n",
      " |          archives, it can only be left as None if an existing archive is being read back in.\n",
      " |      :param bool clobber: If an archive named 'archive_name' already exists, then setting clobber to True\n",
      " |          will cause it to be deleted and overwritten.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Called when initiating iterating through an archive. Resets the counter _n.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      The result of using the Python len() command on this archive - the number of missions associated.\n",
      " |      \n",
      " |      :return: The number of missions in this archive.\n",
      " |      :rtype: int\n",
      " |  \n",
      " |  __next__(self)\n",
      " |      Iterates the counter _n uses it to find the name of the corresponding mission, then retrieves\n",
      " |      that source from the _missions dictionary. Missions are accessed using their name as a key.\n",
      " |  \n",
      " |  check_dependence_success(self, mission_name: str, obs_ident: Union[str, List[str], List[List[str]]], dep_proc: Union[str, List[str]], no_success_error: bool = True) -> numpy.ndarray\n",
      " |      This method should be used by processing functions, rather than the user, to determine whether previous\n",
      " |      processing steps (specified in the input to this function) ran successfully for the specified data.\n",
      " |      \n",
      " |      Each processing function should be setup to call this method with appropriate previous steps and\n",
      " |      identifiers, and will know from its boolean array return which data can be processed safely. If no data\n",
      " |      has successfully run through a previous step, or no attempt to run a previous step occurred, then an\n",
      " |      error will be thrown.\n",
      " |      \n",
      " |      :param str mission_name: The name of the mission for which we wish to check the success of\n",
      " |          previous processing steps.\n",
      " |      :param str/List[str], List[List[str]] obs_ident: A set (or individual) set of observation identifiers. This\n",
      " |          should be in the style output by get_obs_to_process (i.e. [ObsID, Inst, SubExp (depending on mission)],\n",
      " |          though does also support just an ObsID.\n",
      " |      :param str/List[str] dep_proc: The name(s) of the process(es) that have to have been run for further\n",
      " |          processing steps to be successful.\n",
      " |      :param bool no_success_error: If none of the specified previous processing steps have been run\n",
      " |          successfully, should a NoDependencyProcessError be raised. Default is True, but if set to False the\n",
      " |          error will not be raised and the return will be an all-False array. This will NOT override the\n",
      " |          error raised if a previous process hasn't been run at all.\n",
      " |      :return: A boolean array that defines whether the process(es) specified in the input were successful. Each\n",
      " |          set of identifying information provided in obs_ident has a corresponding entry in the return.\n",
      " |      :rtype: np.ndarray\n",
      " |  \n",
      " |  construct_failed_data_path(self, mission: Union[daxa.mission.base.BaseMission, str] = None, obs_id: str = None) -> str\n",
      " |      This method is to construct paths to directories where data for a particular mission + observation\n",
      " |      ID combination which failed to process will be stored. That functionality is added here so that any change\n",
      " |      to how those directories are named will take place in only one part of DAXA, and will propagate to other\n",
      " |      parts of the module. It is unlikely that a user will need to directly use this method.\n",
      " |      \n",
      " |      If no mission is passed, then no observation ID may be passed. In the case of 'mission' and 'obs_id' being\n",
      " |      None, the returned string will be constructed ready to format; {mn} should be replaced by the DAXA mission\n",
      " |      name, and {oi} by the relevant ObsID.\n",
      " |      \n",
      " |      Retrieving a data path from this method DOES NOT guarantee that it has been created.\n",
      " |      \n",
      " |      :param BaseMission/str mission: The mission for which to retrieve the failed data path. Default is None\n",
      " |          in which case a path ready to be formatted with a mission name will be provided.\n",
      " |      :param str obs_id: The ObsID for which to retrieve the failed data path, cannot be set if 'mission' is\n",
      " |          set to None. Default is None, in which case a path ready to be formatted with an observation ID will\n",
      " |          be provided.\n",
      " |      :return: The requested path.\n",
      " |      :rtype: str\n",
      " |  \n",
      " |  construct_processed_data_path(self, mission: Union[daxa.mission.base.BaseMission, str] = None, obs_id: str = None) -> str\n",
      " |      This method is to construct paths to directories where processed data for a particular mission + observation\n",
      " |      ID combination will be stored. That functionality is added here so that any change to how those directories\n",
      " |      are named will take place in only one part of DAXA, and will propagate to other parts of the module. It is\n",
      " |      unlikely that a user will need to directly use this method.\n",
      " |      \n",
      " |      If no mission is passed, then no observation ID may be passed. In the case of 'mission' and 'obs_id' being\n",
      " |      None, the returned string will be constructed ready to format; {mn} should be replaced by the DAXA mission\n",
      " |      name, and {oi} by the relevant ObsID.\n",
      " |      \n",
      " |      Retrieving a data path from this method DOES NOT guarantee that it has been created.\n",
      " |      \n",
      " |      :param BaseMission/str mission: The mission for which to retrieve the processed data path. Default is None\n",
      " |          in which case a path ready to be formatted with a mission name will be provided.\n",
      " |      :param str obs_id: The ObsID for which to retrieve the processed data path, cannot be set if 'mission' is\n",
      " |          set to None. Default is None, in which case a path ready to be formatted with an observation ID will\n",
      " |          be provided.\n",
      " |      :return: The requested path.\n",
      " |      :rtype: str\n",
      " |  \n",
      " |  get_current_data_path(self, mission: Union[daxa.mission.base.BaseMission, str], obs_id: str) -> str\n",
      " |      A method which returns the current location of the archive data for a particular ObsID of a particular\n",
      " |      mission. The two location options are in the 'processed' directory, which is the default and will be the\n",
      " |      home of all ObsIDs that haven't made it to the final process for a particular mission, or the 'failed'\n",
      " |      directory, where any ObsID that has no use (per the final checks) will be stored.\n",
      " |      \n",
      " |      :param BaseMission/str mission: The mission for which to retrieve the current data path.\n",
      " |      :param str obs_id: The ObsID for which to retrieve the current data path.\n",
      " |      :return: The current path to the requested ObsID of the specified mission.\n",
      " |      :rtype: str\n",
      " |  \n",
      " |  get_failed_logs(self, process_name: str) -> Tuple[dict, dict]\n",
      " |      A convenience method that retrieves the logs (stdout and stderr) for processing of particular data (be it a\n",
      " |      whole ObsID, a  particular instrument of an ObsID, or a particular sub-exposure of a particular instrument\n",
      " |      of an ObsID) which FAILED.\n",
      " |      \n",
      " |      :param str process_name: The process for which logs (stdout and stderr) are to be retrieved if the data of\n",
      " |          a particular unique identifier failed.\n",
      " |      :return: A tuple of two dictionaries, the first containing stdout logs, and the second containing stderr\n",
      " |          logs - the structure of the dictionaries has mission names as top level keys, unique identifiers as lower\n",
      " |          level keys, and string logs as values.\n",
      " |      :rtype: Tuple[dict, dict]\n",
      " |  \n",
      " |  get_failed_processes(self, process_name: str) -> dict\n",
      " |      A simple method to retrieve all unique identifiers of data that failed a particular processing step. The\n",
      " |      names of processes that have been run can be found in the 'process_names' property of an Archive.\n",
      " |      \n",
      " |      :param str process_name: The process for which unique identifiers of data that failed the processing step\n",
      " |          are to be retrieved (see 'process_names' property for the names of processes run on this archive).\n",
      " |      :return: A dictionary, with mission names as top level keys, and values being lists of failed\n",
      " |          unique identifiers.\n",
      " |      :rtype: dict\n",
      " |  \n",
      " |  get_obs_to_process(self, mission_name: str, search_ident: str = None) -> List[List[str]]\n",
      " |      This method will provide a list of lists of [ObsID, Instrument, SubExposure (depending on mission)] that\n",
      " |      should be processed for scientific use for a specific mission. The idea is that this method can be\n",
      " |      called, and just by iterating through the result you will get the identifiers of all valid data that\n",
      " |      match your input.\n",
      " |      \n",
      " |      It shouldn't really need to be used directly by users, but instead will be very useful for the processing\n",
      " |      functions - it will tell them which data need to be processed.\n",
      " |      \n",
      " |      :param str mission_name: The internal DAXA name of the mission to retrieve information for.\n",
      " |      :param str search_ident: Either an ObsID or an instrument name to retrieve matching information for. An ObsID\n",
      " |          will search through all the instruments/subexposures, an instrument will search all ObsIDs and\n",
      " |          sub-exposures. The default is None, in which case all ObsIDs, instruments, and sub-exposures will\n",
      " |          be searched.\n",
      " |      :return: List of lists of [ObsID, Instrument, SubExposure (depending on mission)].\n",
      " |      :rtype: List[List]\n",
      " |  \n",
      " |  get_process_logs(self, process_name: str, mission_name: Union[str, List[str]] = None, obs_id: Union[str, List[str]] = None, inst: Union[str, List[str]] = None, full_ident: Union[str, List[str]] = None) -> dict\n",
      " |      This method allows for targeted retrieval of processing logs (stdout), for a specific processing step. The\n",
      " |      particular logs retrieved can be narrows down by mission, ObsID, or instrument. Multiple missions, ObsIDs, and\n",
      " |      instruments may be specified, but only one process at a time. The names of processes that have been run can\n",
      " |      be found in the 'process_names' property of an Archive.\n",
      " |      \n",
      " |      :param str process_name: The process for which logs are to be retrieved (see 'process_names' property for\n",
      " |          the names of processes run on this archive).\n",
      " |      :param str/List[str] mission_name: The mission name(s) for which logs are to be retrieved. Default is None, in\n",
      " |          which case all missions will be searched, and either a single name or a list of names can be passed. See\n",
      " |          'mission_names' for a list of associated mission names.\n",
      " |      :param str/List[str] obs_id: The ObsID(s) for which logs are to be retrieved. Default is None, in which case\n",
      " |          all ObsIDs will be searched. Either a single or a set of ObsIDs can be passed.\n",
      " |      :param str/List[str] inst: The instrument(s) for which logs are to be retrieved. Default is None, in which case\n",
      " |          all instruments will be searched. Either a single or a set of instruments can be passed.\n",
      " |      :param str/List[str] full_ident: A full unique identifier (or a set of them) to make matches too. This will\n",
      " |          override any ObsID or insts that are specified - for instance one could pass 0201903501PNS003. Default is\n",
      " |          None.\n",
      " |      :return: A dictionary containing the requested logs - top level keys are mission names, lower level keys are\n",
      " |          unique identifiers, and the values are string logs which match the provided information.\n",
      " |      :rtype: dict\n",
      " |  \n",
      " |  get_process_raw_error_logs(self, process_name: str, mission_name: Union[str, List[str]] = None, obs_id: Union[str, List[str]] = None, inst: Union[str, List[str]] = None, full_ident: Union[str, List[str]] = None) -> dict\n",
      " |      This method allows for targeted retrieval of processing raw-error logs (stderr), for a specific processing\n",
      " |      step. The particular logs retrieved can be narrows down by mission, ObsID, or instrument. Multiple missions,\n",
      " |      ObsIDs, and instruments may be specified, but only one process at a time. The names of processes that have\n",
      " |      been run can be found in the 'process_names' property of an Archive.\n",
      " |      \n",
      " |      :param str process_name: The process for which logs are to be retrieved (see 'process_names' property for\n",
      " |          the names of processes run on this archive).\n",
      " |      :param str/List[str] mission_name: The mission name(s) for which logs are to be retrieved. Default is None, in\n",
      " |          which case all missions will be searched, and either a single name or a list of names can be passed. See\n",
      " |          'mission_names' for a list of associated mission names.\n",
      " |      :param str/List[str] obs_id: The ObsID(s) for which logs are to be retrieved. Default is None, in which case\n",
      " |          all ObsIDs will be searched. Either a single or a set of ObsIDs can be passed.\n",
      " |      :param str/List[str] inst: The instrument(s) for which logs are to be retrieved. Default is None, in which case\n",
      " |          all instruments will be searched. Either a single or a set of instruments can be passed.\n",
      " |      :param str/List[str] full_ident: A full unique identifier (or a set of them) to make matches too. This will\n",
      " |          override any ObsID or insts that are specified - for instance one could pass 0201903501PNS003. Default is\n",
      " |          None.\n",
      " |      :return: A dictionary containing the requested logs - top level keys are mission names, lower level keys are\n",
      " |          unique identifiers, and the values are string logs which match the provided information.\n",
      " |      :rtype: dict\n",
      " |  \n",
      " |  get_region_file_path(self, mission: Union[daxa.mission.base.BaseMission, str] = None, obs_id: str = None) -> str\n",
      " |      This method is to construct paths to files where the regions associated with a particular observation of a\n",
      " |      particular mission are stored after being added to the archive. If a mission and ObsID are specified then\n",
      " |      this method will check whether region information for that particular ObsID of that particular mission\n",
      " |      exists in this archive, and raise an error if it does not.\n",
      " |      \n",
      " |      If no mission is passed, then no observation ID may be passed. In the case of 'mission' and 'obs_id' being\n",
      " |      None, the returned string will be constructed ready to format; {mn} should be replaced by the DAXA mission\n",
      " |      name, and {oi} by the relevant ObsID.\n",
      " |      \n",
      " |      Retrieving a region file path from this method without passing mission and ObsID DOES NOT guarantee that one\n",
      " |      has been created for whatever mission and ObsID are added to the string later.\n",
      " |      \n",
      " |      :param BaseMission/str mission: The mission for which to retrieve the region file path. Default is None\n",
      " |          in which case a path ready to be formatted with a mission name will be provided.\n",
      " |      :param str obs_id: The ObsID for which to retrieve the region file path, cannot be set if 'mission' is\n",
      " |          set to None. Default is None, in which case a path ready to be formatted with an observation ID will\n",
      " |          be provided.\n",
      " |      :return: The requested path.\n",
      " |      :rtype: str\n",
      " |  \n",
      " |  info(self)\n",
      " |      A simple method to present summary information about this archive.\n",
      " |  \n",
      " |  save(self)\n",
      " |      A simple method that saves the information necessary to reload this archive from disk at a later time. This\n",
      " |      largely consists of the various pieces of information regarding the success (or not) of various processing\n",
      " |      steps.\n",
      " |      \n",
      " |      NOTE that the mission states are not saved here, as they could be triggered repeatedly, which can be slow\n",
      " |      for the ones with many possible ObsIDs (i.e. Swift and Integral). Instead, saves are triggered when the archive\n",
      " |      is created, in the init, and if the data in the archive are updated (as this necessitates a change in the\n",
      " |      mission states).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  archive_name\n",
      " |      Property getter for the name assigned to this archive by the user.\n",
      " |      :return: The archive name.\n",
      " |      :rtype: str\n",
      " |  \n",
      " |  archive_path\n",
      " |      The property getter for the absolute path to the output archive directory.\n",
      " |      \n",
      " |      :return: Absolute path to the archive.\n",
      " |      :rtype: str\n",
      " |  \n",
      " |  mission_names\n",
      " |      Property getter for the names of the missions associated with this Archive.\n",
      " |      \n",
      " |      :return:\n",
      " |      :rtype: List[str]\n",
      " |  \n",
      " |  missions\n",
      " |      Property getter that returns either a list of missions associated with this Archive, or a single\n",
      " |      mission associated with this Archive (if only one mission was supplied).\n",
      " |      \n",
      " |      :return: Missions (or mission) associated with this archive.\n",
      " |      :rtype: Union[List[BaseMission], BaseMission]\n",
      " |  \n",
      " |  process_names\n",
      " |      Property that returns a dictionary containing the names of all processing steps that have been run on this\n",
      " |      archive. Top-level keys are mission names, and the values are lists of process names.\n",
      " |      \n",
      " |      :return: The dictionary containing mission name and process name information. Top-level keys are mission\n",
      " |          names, and the values are lists of process names.\n",
      " |      :rtype: dict\n",
      " |  \n",
      " |  process_observation\n",
      " |      This property returns the dictionary of mission-ObsID-Instrument(-subexposure) boolean flags that indicate\n",
      " |      whether the data for that observation-instrument(-subexposure) should be processed for science. There is a\n",
      " |      companion get method that returns only the data identifiers that should be processed.\n",
      " |      \n",
      " |      :return: The dictionary containing information on whether particular data should be processed.\n",
      " |      :rtype: dict\n",
      " |  \n",
      " |  top_level_path\n",
      " |      The property getter for the absolute path to the top-level DAXA storage directory.\n",
      " |      \n",
      " |      :return: Absolute top-level storage path.\n",
      " |      :rtype: str\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  final_process_success\n",
      " |      This property returns the dictionary which stores the final judgement (at the ObsID level) of whether there\n",
      " |      are any useful data (True) or whether no aspect of that observation reached the end of the final processing\n",
      " |      step successfully. The ObsIDs marked as False will be moved from the archive processed data directory to a\n",
      " |      separate failed data directory.\n",
      " |      \n",
      " |      The flags are only added once the final processing step for a particular mission has been run.\n",
      " |      \n",
      " |      :return: The dictionary of final processing success flags.\n",
      " |      :rtype: dict\n",
      " |  \n",
      " |  observation_summaries\n",
      " |      This property returns information on the different observations available to each mission. This information\n",
      " |      will vary from mission to mission, and is primarily intended for use by DAXA processing methods, but could\n",
      " |      include things such as whether an instrument was active for a particular observation, what sub-exposures\n",
      " |      there were (relevant for XMM for instance), what filter was active, etc.\n",
      " |      \n",
      " |      :return: A dictionary of information with missions as the top level keys, then ObsIDs, then instruments.\n",
      " |          Keys on levels below that will be determined by the information available for specific instruments of\n",
      " |          specific missions.\n",
      " |      :rtype: bool\n",
      " |  \n",
      " |  process_errors\n",
      " |      Property getter for a nested dictionary containing error information from processing applied to mission data.\n",
      " |      \n",
      " |      :return: A nested dictionary where top level keys are mission names, next level keys are processing\n",
      " |          function names, and lowest level keys are either ObsID or ObsID+instrument names. The values\n",
      " |          attributed with the lowest level keys are error outputs (e.g. parsed from stderr from command line\n",
      " |          tools).\n",
      " |      :rtype: dict\n",
      " |  \n",
      " |  process_extra_info\n",
      " |      Property getter for a nested dictionary containing extra information from processing applied to mission data.\n",
      " |      This can be things like paths to event lists, or configuration information. It is unlikely to be necessary\n",
      " |      for users to directly access this property.\n",
      " |      \n",
      " |      :return: A nested dictionary where top level keys are mission names, next level keys are processing\n",
      " |          function names, and lowest level keys are either ObsID or ObsID+instrument names. The values\n",
      " |          attributed with the lowest level keys are dictionaries of extra information (e.g. config info).\n",
      " |      :rtype: dict\n",
      " |  \n",
      " |  process_logs\n",
      " |      Property getter for a nested dictionary containing log information from processing applied to mission data.\n",
      " |      \n",
      " |      :return: A nested dictionary where top level keys are mission names, next level keys are processing\n",
      " |          function names, and lowest level keys are either ObsID or ObsID+instrument names. The values\n",
      " |          attributed with the lowest level keys are logs (e.g. stdout from command line tools).\n",
      " |      :rtype: dict\n",
      " |  \n",
      " |  process_success\n",
      " |      Property getter for a nested dictionary containing boolean flags describing whether different processing steps\n",
      " |      applied to observations from various missions are considered to have completed successfully.\n",
      " |      \n",
      " |      :return: A nested dictionary where top level keys are mission names, next level keys are processing\n",
      " |          function names, and lowest level keys are either ObsID or ObsID+instrument names. The values\n",
      " |          attributed with the lowest level keys are boolean, with True indicating that the processing function\n",
      " |          was successful\n",
      " |      :rtype: dict\n",
      " |  \n",
      " |  process_warnings\n",
      " |      Property getter for a nested dictionary containing warning information from processing applied to mission data.\n",
      " |      \n",
      " |      :return: A nested dictionary where top level keys are mission names, next level keys are processing\n",
      " |          function names, and lowest level keys are either ObsID or ObsID+instrument names. The values\n",
      " |          attributed with the lowest level keys are warning outputs (e.g. parsed from stderr from command line\n",
      " |          tools).\n",
      " |      :rtype: dict\n",
      " |  \n",
      " |  raw_process_errors\n",
      " |      Property getter for a nested dictionary containing unparsed error information (e.g. the entire stderr\n",
      " |      output from an XMM SAS process) from processing applied to mission data.\n",
      " |      \n",
      " |      :return: A nested dictionary where top level keys are mission names, next level keys are processing\n",
      " |          function names, and lowest level keys are either ObsID or ObsID+instrument names. The values\n",
      " |          attributed with the lowest level keys are error outputs (e.g. stderr from command line\n",
      " |          tools).\n",
      " |      :rtype: dict\n",
      " |  \n",
      " |  source_regions\n",
      " |      This property returns all source regions which have been associated with missions in this archive. The top\n",
      " |      level keys of the dictionary are mission names, the bottom level keys are observation identifiers, and the\n",
      " |      values are lists of region objects.\n",
      " |      \n",
      " |      If an observation in this archive has had regions added for it, then those regions will also have been\n",
      " |      written to permanent storage in the archive directory structure. The path can be identified using the\n",
      " |      get_region_file_path method of this archive.\n",
      " |      \n",
      " |      :return: Dictionary containing regions on a mission-observation basis.\n",
      " |      :rtype: dict\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Archive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c60fde",
   "metadata": {},
   "source": [
    "## Archive properties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6101cd2e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b605eaf1",
   "metadata": {},
   "source": [
    "### ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cbd34e",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
