{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2fc5e39",
   "metadata": {},
   "source": [
    "# Creating and interacting with a DAXA Archive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ec471d",
   "metadata": {},
   "source": [
    "This tutorial will explain the basic concepts behind the second type of important class in DAXA, the Archive class (with mission classes being the first type, see [the missions tutorial](missions.html)). DAXA Archives are what manage the datasets that we download from various missions, enabling easy access and greatly simplifying processing/reduction - they allow you to stop thinking about all the files and settings that any large dataset entails.\n",
    "\n",
    "We will cover the following:\n",
    "\n",
    "* Setting up an Archive from scratch, using filtered DAXA missions.\n",
    "* Loading an existing Archive from disk.\n",
    "* The properties of an Archive.\n",
    "* Accessing processing logs and success information (though we do not cover processing in this part of the documentation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cadd97e",
   "metadata": {},
   "source": [
    "## Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f08d005d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from daxa.mission import XMMPointed, Chandra, eRASS1DE, ROSATPointed\n",
    "from daxa.archive import Archive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13eeee76",
   "metadata": {},
   "source": [
    "## What is a DAXA archive?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cc2c89",
   "metadata": {},
   "source": [
    "DAXA Archives take a set of filtered missions, make sure that their data are downloaded, and enable easy access and organisation of all data files and processing functions. Key functionality includes:\n",
    "\n",
    "* Storing the logs and errors of all processing steps (if run).\n",
    "* Allowing for their easy retrieval. \n",
    "* Managing the myriad files produced during the processing.\n",
    "* Keeping track of which processes failed for which data, ensuring that any further processing only runs on data that have successfully passed through the earlier processes.\n",
    "\n",
    "Archives can also be loaded back into DAXA at a later date, so that the processing logs of data that has since been found to be problematic can be easily inspected, or indeed so that processing steps can be re-run with different settings; this also allows for archives to be updated, if more data become available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03822d2",
   "metadata": {},
   "source": [
    "## Creating a new archive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc71354",
   "metadata": {},
   "source": [
    "Here we will demonstrate how to set up a new DAXA Archive from scratch - this information can be combined with the [the missions tutorial](missions.html) and the <font color='red'>case studies</font> to create an archive from any dataset you might be using."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f62da9",
   "metadata": {},
   "source": [
    "### Step 1 - Set up and filter missions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54683039",
   "metadata": {},
   "source": [
    "The first thing we have to do is to select the observations that we wish to include in the archive (and indeed the missions that we wish to include). The missions all have different characteristics, so your choice of which to include will be heavily dependent on your science case.\n",
    "\n",
    "Here we will create an archive of XMM, Chandra, eROSITA All-Sky DR1, and ROSAT pointed observations of a famous galaxy cluster (though the archive would behave the same if it held data for a large sample of objects).\n",
    "\n",
    "First of all, we define instances of the mission classes that we wish to include:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c420fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dt237/code/DAXA/daxa/mission/xmm.py:83: UserWarning: 140 of the 17678 observations located for this mission have been removed due to NaN RA or Dec values\n",
      "  self._fetch_obs_info()\n"
     ]
    }
   ],
   "source": [
    "xm = XMMPointed()\n",
    "ch = Chandra()\n",
    "er = eRASS1DE()\n",
    "rp = ROSATPointed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5545e7",
   "metadata": {},
   "source": [
    "Then we filter them to only include observations of our cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a7f2e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dt237/code/DAXA/daxa/mission/base.py:1075: UserWarning: Chandra FoV are difficult to define, as they can be strongly dependant on observation mode; as such take these as very approximate.\n",
      "  fov = self.fov\n"
     ]
    }
   ],
   "source": [
    "xm.filter_on_name(\"A3667\")\n",
    "ch.filter_on_name(\"A3667\")\n",
    "er.filter_on_name(\"A3667\")\n",
    "rp.filter_on_name(\"A3667\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425fdd56",
   "metadata": {},
   "source": [
    "We then download the available data (though the declaration of an Archive would also trigger this, we do it this way because we wish to download pre-generated products for Chandra and ROSAT pointed observations):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec7051f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/td/gw9qkx6d3szb1nkt_cfvcbzm000vzl/T/ipykernel_63028/3764573067.py:1: UserWarning: The raw data for this mission have already been downloaded.\n",
      "  xm.download()\n",
      "/var/folders/td/gw9qkx6d3szb1nkt_cfvcbzm000vzl/T/ipykernel_63028/3764573067.py:2: UserWarning: The raw data for this mission have already been downloaded.\n",
      "  ch.download(download_products=True)\n",
      "/var/folders/td/gw9qkx6d3szb1nkt_cfvcbzm000vzl/T/ipykernel_63028/3764573067.py:3: UserWarning: The raw data for this mission have already been downloaded.\n",
      "  er.download()\n",
      "/Users/dt237/code/DAXA/daxa/mission/rosat.py:594: UserWarning: The raw data for this mission have already been downloaded.\n",
      "  warn(\"The raw data for this mission have already been downloaded.\")\n"
     ]
    }
   ],
   "source": [
    "xm.download()\n",
    "ch.download(download_products=True)\n",
    "er.download()\n",
    "rp.download(download_products=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a29ae12",
   "metadata": {},
   "source": [
    "### Step 2 - Setting up an Archive object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba56bd2",
   "metadata": {},
   "source": [
    "Now we create the actual DAXA Archive instance - all this requires is for us to choose an archive name (which is what will be used to load it back in at a later date, if necessary) and to pass in the filtered missions that we have already created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b038833",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/td/gw9qkx6d3szb1nkt_cfvcbzm000vzl/T/ipykernel_63028/1119153751.py:1: UserWarning: An archive called A3667 already existed, but as 'clobber=True' it has been deleted and overwritten.\n",
      "  arch = Archive(\"A3667\", [xm, ch, er, rp], clobber=True)\n"
     ]
    }
   ],
   "source": [
    "arch = Archive(\"A3667\", [xm, ch, er, rp], clobber=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea6658d",
   "metadata": {},
   "source": [
    "Now we've declared it, we can use the `info()` method to get a summary of its current status, including the amount of data available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "310ef5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------------------------------------------------\n",
      "Number of missions - 4\n",
      "Total number of observations - 24\n",
      "Beginning of earliest observation - 1992-04-14 18:55:38.000003\n",
      "End of latest observation - 2020-04-20 12:23:50\n",
      "\n",
      "-- XMM-Newton Pointed --\n",
      "   Internal DAXA name - xmm_pointed\n",
      "   Chosen instruments - M1, M2, PN\n",
      "   Number of observations - 8\n",
      "   Fully Processed - False\n",
      "\n",
      "-- Chandra --\n",
      "   Internal DAXA name - chandra\n",
      "   Chosen instruments - ACIS-I, ACIS-S, HRC-I, HRC-S\n",
      "   Number of observations - 12\n",
      "   Fully Processed - False\n",
      "\n",
      "-- eRASS DE:1 --\n",
      "   Internal DAXA name - erosita_all_sky_de_dr1\n",
      "   Chosen instruments - TM1, TM2, TM3, TM4, TM5, TM6, TM7\n",
      "   Number of observations - 1\n",
      "   Fully Processed - False\n",
      "\n",
      "-- ROSAT Pointed --\n",
      "   Internal DAXA name - rosat_pointed\n",
      "   Chosen instruments - PSPCB, PSPCC, HRI\n",
      "   Number of observations - 3\n",
      "   Fully Processed - False\n",
      "-----------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "arch.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35195e4",
   "metadata": {},
   "source": [
    "### Step 3 - Processing the Archive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe5b46b",
   "metadata": {},
   "source": [
    "We're not actually going to cover _how_ to process things here, as each telescope tends to have its own backend software with a unique way of doing things; they each have their own processing tutorials, which will demonstrate both a one-line processing method, and how to control the reduction in more detail. Any processing method will take the archive object as an argument, and act on the data stored within it.\n",
    "\n",
    "So instead we include this step here to highlight that the next logical step after the creation of a new archive is to run processing and reduction routines, if raw data have been downloaded. The successful completion of this step will leave you with an archive of data that you can easily manage, access, and use for your scientific analyses.\n",
    "\n",
    "If you elected to download existing products (most missions support this), then only one processing step is necessary - this reorganises the downloaded data so that it is compatible with DAXA storage and file naming conventions. **It will have run automatically on declaration**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b220387",
   "metadata": {},
   "source": [
    "### Note on saving Archives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be92468c",
   "metadata": {},
   "source": [
    "Archive instances can be saved and loaded back in (as you'll see in the next section). This can be triggered manually by running the `save()` method, but __this shouldn't be generally necessary__ - this is because the archive is automatically saved upon first setup, and after every processing step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b1269a",
   "metadata": {},
   "source": [
    "## Loading an existing archive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44310223",
   "metadata": {},
   "source": [
    "As we have intimated, previously created archives can be loaded back in to memory in exactly the same state as when they were saved. We will demonstrate this here with an archive we prepared earlier - it has had XMM processing applied, which will allow us to demonstrate the logging and management functionality. \n",
    "\n",
    "Reloading an archive has a number of possible applications:\n",
    "\n",
    "* Access to archive data management functions - e.g. locating specific data files, identifying what observations are available.\n",
    "* Checking processing logs - e.g. finding errors or warnings in the processing of data that has since been identified as problematic.\n",
    "* Updating the archive - either adding another mission, or using the archive to check for new data matching your original mission filtering operations (these are stored in the mission saves, so can be re-run automatically).\n",
    "\n",
    "All you need to do is set up an Archive instance and pass the name of an existing archive - this assumes your code is running in the same directory as it was originally, as Archives are stored in 'daxa_output' (if the DAXA configuration file hasn't been altered). The configuration can also be altered so that all DAXA outputs are stored in an absolute path, in which case defining an Archive object with the name of an existing dataset would work from any directory).\n",
    "\n",
    "Loading in an archive (note that you don't need to pass any missions, loading the archive back in will also reinstate the missions as they were when the Archive was last saved):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fccbc3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dt237/code/DAXA/daxa/mission/xmm.py:83: UserWarning: 140 of the 17678 observations located for this mission have been removed due to NaN RA or Dec values\n",
      "  self._fetch_obs_info()\n"
     ]
    }
   ],
   "source": [
    "prev_arch = Archive(\"A3667_made_earlier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e74ad9",
   "metadata": {},
   "source": [
    "Once again, we will run the `info()` method, but note that for this archive the XMM-Newton Pointed mission is marked as 'fully processed':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a4f8874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------------------------------------------------\n",
      "Number of missions - 4\n",
      "Total number of observations - 31147\n",
      "Beginning of earliest observation - 1990-06-16 21:07:02.999997\n",
      "End of latest observation - 2024-04-08 15:21:21.000002\n",
      "\n",
      "-- XMM-Newton Pointed --\n",
      "   Internal DAXA name - xmm_pointed\n",
      "   Chosen instruments - M1, M2, PN\n",
      "   Number of observations - 8\n",
      "   Fully Processed - True\n",
      "\n",
      "-- Chandra --\n",
      "   Internal DAXA name - chandra\n",
      "   Chosen instruments - ACIS-I, ACIS-S, HRC-I, HRC-S\n",
      "   Number of observations - 20263\n",
      "   Fully Processed - False\n",
      "\n",
      "-- eRASS DE:1 --\n",
      "   Internal DAXA name - erosita_all_sky_de_dr1\n",
      "   Chosen instruments - TM1, TM2, TM3, TM4, TM5, TM6, TM7\n",
      "   Number of observations - 1\n",
      "   Fully Processed - False\n",
      "\n",
      "-- ROSAT Pointed --\n",
      "   Internal DAXA name - rosat_pointed\n",
      "   Chosen instruments - PSPCB, PSPCC, HRI\n",
      "   Number of observations - 10875\n",
      "   Fully Processed - False\n",
      "-----------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prev_arch.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc450378",
   "metadata": {},
   "source": [
    "We note that it _is_ possible to declare an Archive with a previously used name and overwrite it - you just have to pass `clobber=True` when you declare the Archive instance. We print the docstring of the Archive class here for reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bec2e2c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    The Archive class, which is to be used to consolidate and provide some interface with a set\n",
      "    of mission's data. Archives can be passed to processing and cleaning functions in DAXA, and also\n",
      "    contain convenience functions for accessing summaries of the available data.\n",
      "\n",
      "    :param str archive_name: The name to be given to this archive - it will be used for storage\n",
      "        and identification. If an existing archive with this name exists it will be read in, unless clobber=True.\n",
      "    :param List[BaseMission]/BaseMission missions: The mission, or missions, which are to be included\n",
      "        in this archive - any setup processes (i.e. the filtering of data to be acquired) should be\n",
      "        performed prior to creating an archive. The default value is None, but this should be set for any new\n",
      "        archives, it can only be left as None if an existing archive is being read back in.\n",
      "    :param bool clobber: If an archive named 'archive_name' already exists, then setting clobber to True\n",
      "        will cause it to be deleted and overwritten.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(Archive.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c60fde",
   "metadata": {},
   "source": [
    "## Archive properties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6101cd2e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b605eaf1",
   "metadata": {},
   "source": [
    "### ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cbd34e",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
